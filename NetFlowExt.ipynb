{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Xiaohu-cqu/NeuralNet_nilm/blob/master/NetFlowExt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VCLJ523QmUA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorlayer as tl\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "def dict_to_one(dp_dict={}):\n",
        "\n",
        "    \"\"\" Input a dictionary, return a dictionary that all items are\n",
        "    set to one, use for disable dropout, drop-connect layer and so on.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    dp_dict : dictionary keeping probabilities date\n",
        "    \"\"\"\n",
        "    return {x: 1 for x in dp_dict}\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "\n",
        "    return 1/(1+np.exp(-x))\n",
        "\n",
        "\n",
        "def modelsaver(network, path, epoch_identifier=None):\n",
        "\n",
        "    if epoch_identifier:\n",
        "        ifile = path + '_' + str(epoch_identifier)+'.npz'\n",
        "    else:\n",
        "        ifile = path + '.npz'\n",
        "    tl.files.save_npz(network.all_params, name=ifile)\n",
        "\n",
        "\n",
        "def customfit(sess, \n",
        "              network, \n",
        "              cost, \n",
        "              train_op, \n",
        "              tra_provider, \n",
        "              x, \n",
        "              y_, \n",
        "              acc=None, \n",
        "              n_epoch=50,\n",
        "              print_freq=1, \n",
        "              val_provider=None, \n",
        "              save_model=-1, \n",
        "              tra_kwag=None, \n",
        "              val_kwag=None,\n",
        "              save_path=None, \n",
        "              epoch_identifier=None, \n",
        "              earlystopping=True, \n",
        "              min_epoch=10,\n",
        "              patience=10):\n",
        "    \"\"\"\n",
        "        Traing a given network by the given cost function, dataset, n_epoch etc.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        sess : TensorFlow session\n",
        "            sess = tf.InteractiveSession()\n",
        "        network : a TensorLayer layer\n",
        "            the network will be trained\n",
        "        train_op : a TensorFlow optimizer\n",
        "            like tf.train.AdamOptimizer\n",
        "        x : placeholder\n",
        "            for inputs\n",
        "        y_ : placeholder\n",
        "            for targets\n",
        "        acc : the TensorFlow expression of accuracy (or other metric) or None\n",
        "            if None, would not display the metric\n",
        "        batch_size : int\n",
        "            batch size for training and evaluating\n",
        "        n_epoch : int\n",
        "            the number of training epochs\n",
        "        print_freq : int\n",
        "            display the training information every ``print_freq`` epochs\n",
        "        X_val : numpy array or None\n",
        "            the input of validation data\n",
        "        y_val : numpy array or None\n",
        "            the target of validation data\n",
        "        eval_train : boolen\n",
        "            if X_val and y_val are not None, it refects whether to evaluate the training data\n",
        "    \"\"\"\n",
        "    # parameters for earlystopping\n",
        "    best_valid = np.inf\n",
        "    best_valid_acc = np.inf\n",
        "    best_valid_epoch = min_epoch\n",
        "    \n",
        "    # assert X_train.shape[0] >= batch_size, \"Number of training examples should be bigger than the batch size\"\n",
        "    print(\"Start training the network ...\")\n",
        "    start_time_begin = time.time()\n",
        "    for epoch in range(n_epoch):\n",
        "        #start_time = time.time()\n",
        "        loss_ep = 0;\n",
        "        n_step = 0\n",
        "\n",
        "        for batch in tra_provider.feed(**tra_kwag):\n",
        "            X_train_a, y_train_a = batch\n",
        "            feed_dict = {x: X_train_a, y_: y_train_a}\n",
        "            feed_dict.update(network.all_drop)  # enable noise layers\n",
        "            loss, _ = sess.run([cost, train_op], feed_dict=feed_dict)\n",
        "            loss_ep += loss\n",
        "            n_step += 1\n",
        "        loss_ep = loss_ep / n_step\n",
        "\n",
        "        if epoch >= 0 or (epoch + 1) % print_freq == 0:\n",
        "            # evaluate the val error at each epoch.\n",
        "            if val_provider is not None:\n",
        "                #print(\"Epoch %d of %d took %fs\" % (epoch + 1, n_epoch, time.time() - start_time))\n",
        "                train_loss, train_acc, n_batch_train = 0, 0, 0\n",
        "                for batch in tra_provider.feed(**tra_kwag):\n",
        "                    X_train_a, y_train_a = batch\n",
        "                    dp_dict = dict_to_one(network.all_drop)  # disable noise layers\n",
        "                    feed_dict = {x: X_train_a, y_: y_train_a}\n",
        "                    feed_dict.update(dp_dict)\n",
        "                    if acc is not None:\n",
        "                        err, ac = sess.run([cost, acc], feed_dict=feed_dict)\n",
        "                        train_acc += ac\n",
        "                    else:\n",
        "                        err = sess.run(cost, feed_dict=feed_dict)\n",
        "                    train_loss += err;\n",
        "                    n_batch_train += 1\n",
        "                #print(\"   train loss: %f\" % (train_loss / n_batch))\n",
        "                # print (train_loss, n_batch)\n",
        "                #if acc is not None:\n",
        "                    #print(\"   train acc: %f\" % (train_acc / n_batch))\n",
        "                val_loss, val_acc, n_batch_val = 0, 0, 0\n",
        "\n",
        "                for batch in val_provider.feed(**val_kwag):\n",
        "                    X_val_a, y_val_a = batch\n",
        "                    dp_dict = dict_to_one(network.all_drop)  # disable noise layers\n",
        "                    feed_dict = {x: X_val_a, y_: y_val_a}\n",
        "                    feed_dict.update(dp_dict)\n",
        "                    if acc is not None:\n",
        "                        err, ac = sess.run([cost, acc], feed_dict=feed_dict)\n",
        "                        val_acc += ac\n",
        "                    else:\n",
        "                        err = sess.run(cost, feed_dict=feed_dict)\n",
        "                    val_loss += err;\n",
        "                    n_batch_val += 1\n",
        "                #print(\"   val loss: %f\" % (val_loss / n_batch))\n",
        "                #if acc is not None:\n",
        "                    #print(\"   val acc: %f\" % (val_acc / n_batch))\n",
        "            #else:\n",
        "                #print(\"Epoch %d of %d took %fs, loss %f\" % (epoch + 1, n_epoch, time.time() - start_time, loss_ep))\n",
        "        \n",
        "        if earlystopping:\n",
        "            if epoch >= min_epoch:\n",
        "                current_valid = val_loss / n_batch_val\n",
        "                current_valid_acc = val_acc / n_batch_val\n",
        "                current_epoch = epoch\n",
        "                current_train_loss = train_loss / n_batch_train\n",
        "                current_train_acc = train_acc / n_batch_train\n",
        "                print('     Current valid loss was {:.6f}, acc was {:.6f}, \\\n",
        "                          train loss was {:.6f}, acc was {:.6f} at epoch {}.'.format(\n",
        "                      current_valid, current_valid_acc, \n",
        "                      current_train_loss, current_train_acc,\n",
        "                      current_epoch))\n",
        "                if current_valid < best_valid:\n",
        "                    best_valid = current_valid\n",
        "                    best_valid_acc = current_valid_acc\n",
        "                    best_valid_epoch = current_epoch\n",
        "                    # save the model parameters\n",
        "                    modelsaver(network=network, path=save_path, epoch_identifier=None)\n",
        "                    print('Best valid loss was {:.6f} and acc {:.6f} at epoch {}.'.format(\n",
        "                          best_valid, best_valid_acc, best_valid_epoch))\n",
        "                elif best_valid_epoch + patience < current_epoch:\n",
        "                    print('Early stopping.')\n",
        "                    print('Best valid loss was {:.6f} and acc {:.6f} at epoch {}.'.format(\n",
        "                          best_valid, best_valid_acc, best_valid_epoch))  \n",
        "                    break\n",
        "                    #raise StopIteration()\n",
        "                \n",
        "        else:                \n",
        "            current_val_loss = val_loss / n_batch_val\n",
        "            current_val_acc = val_acc / n_batch_val\n",
        "            current_epoch = epoch\n",
        "            current_train_loss = train_loss / n_batch_train\n",
        "            current_train_acc = train_acc / n_batch_train\n",
        "            print('     Current valid loss was {:.6f}, acc was {:.6f}, \\\n",
        "                          train loss was {:.6f}, acc was {:.6f} at epoch {}.'.format(\n",
        "                      current_val_loss, current_val_acc, \n",
        "                      current_train_loss, current_train_acc,\n",
        "                      current_epoch))\n",
        "            # print(save_model > 0, epoch % save_model == 0, epoch/save_model > 0)\n",
        "            if save_model > 0 and epoch % save_model == 0:\n",
        "                if epoch_identifier:\n",
        "                    modelsaver(network=network, path=save_path, epoch_identifier=epoch)\n",
        "                else:\n",
        "                    modelsaver(network=network, path=save_path, epoch_identifier=None)\n",
        "    if not earlystopping:        \n",
        "        if save_model == -1:\n",
        "            modelsaver(network=network, path=save_path, epoch_identifier=None)\n",
        "\n",
        "    print(\"Total training time: %fs\" % (time.time() - start_time_begin))\n",
        "\n",
        "\n",
        "def customtest(sess, network, acc, test_provider, x, y_, cost, test_kwag=None):\n",
        "    \"\"\"\n",
        "        Test a given non time-series network by the given test data and metric.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        sess : TensorFlow session\n",
        "            sess = tf.InteractiveSession()\n",
        "        network : a TensorLayer layer\n",
        "            the network will be trained\n",
        "        acc : the TensorFlow expression of accuracy (or other metric) or None\n",
        "            if None, would not display the metric\n",
        "        X_test : numpy array\n",
        "            the input of test data\n",
        "        y_test : numpy array\n",
        "            the target of test data\n",
        "        x : placeholder\n",
        "            for inputs\n",
        "        y_ : placeholder\n",
        "            for targets\n",
        "        batch_size : int or None\n",
        "            batch size for testing, when dataset is large, we should use minibatche for testing.\n",
        "            when dataset is small, we can set it to None.\n",
        "        cost : the TensorFlow expression of cost or None\n",
        "            if None, would not display the cost\n",
        "    \"\"\"\n",
        "    test_loss, test_acc, n_batch = 0, 0, 0\n",
        "    for batch in test_provider.feed(**test_kwag):\n",
        "        X_test_a, y_test_a = batch\n",
        "        dp_dict = dict_to_one(network.all_drop)  # disable noise layers\n",
        "        feed_dict = {x: X_test_a, y_: y_test_a}\n",
        "        feed_dict.update(dp_dict)\n",
        "        if acc is not None:\n",
        "            err, ac = sess.run([cost, acc], feed_dict=feed_dict)\n",
        "            test_acc += ac\n",
        "        else:\n",
        "            err = sess.run(cost, feed_dict=feed_dict)\n",
        "        test_loss += err;\n",
        "        n_batch += 1\n",
        "    print(\"   test loss: %f\" % (test_loss / n_batch))\n",
        "    if acc is not None:\n",
        "        print(\"   test acc: %f\" % (test_acc / n_batch))\n",
        "\n",
        "\n",
        "\n",
        "def custompredict(sess, network, output_provider, x, fragment_size=1000, output_length=1, y_op=None, out_kwag=None):\n",
        "    \"\"\"\n",
        "        Return the predict results of given non time-series network.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        sess : TensorFlow session\n",
        "            sess = tf.InteractiveSession()\n",
        "        network : a TensorLayer layer\n",
        "            the network will be trained\n",
        "        x : placeholder\n",
        "            the input\n",
        "        y_op : placeholder\n",
        "    \"\"\"\n",
        "    dp_dict = dict_to_one(network.all_drop)  # disable noise layers\n",
        "\n",
        "    if y_op is None:\n",
        "        y_op = network.outputs\n",
        "    output_container = []\n",
        "    gt = []\n",
        "    banum = 0\n",
        "    for batch in output_provider.feed(**out_kwag):\n",
        "        # print banum\n",
        "        banum += 1\n",
        "        X_out_a, gt_batch = batch\n",
        "        # print 'hi', X_out_a.mean()\n",
        "        fra_num = X_out_a.shape[0] / fragment_size\n",
        "        offset = X_out_a.shape[0] % fragment_size\n",
        "        final_output = np.zeros((X_out_a.shape[0], output_length))\n",
        "        for fragment in xrange(fra_num):\n",
        "            x_fra = X_out_a[fragment * fragment_size:(fragment + 1) * fragment_size]\n",
        "            feed_dict = {x: x_fra, }\n",
        "            feed_dict.update(dp_dict)\n",
        "            final_output[fragment * fragment_size:(fragment + 1) * fragment_size] = sess.run(y_op, feed_dict=feed_dict).reshape(-1,output_length)\n",
        "\n",
        "        if offset > 0:\n",
        "            feed_dict = {x: X_out_a[-offset:], }\n",
        "            feed_dict.update(dp_dict)\n",
        "            final_output[-offset:] = sess.run(y_op, feed_dict=feed_dict).reshape(-1,output_length)\n",
        "        output_container.append(final_output)\n",
        "        gt.append(gt_batch)\n",
        "        # print 'hello', final_output.mean()\n",
        "    return np.vstack(output_container), np.vstack(gt)\n",
        "\n",
        "\n",
        "def custompredict_add(sess, network, output_provider, x, seqlength, fragment_size=1000, output_length=1,\n",
        "                      y_op=None, out_kwag=None, std=1, mean=0):\n",
        "    \"\"\"\n",
        "        Return the predict results of given non time-series network.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        sess : TensorFlow session\n",
        "            sess = tf.InteractiveSession()\n",
        "        network : a TensorLayer layer\n",
        "            the network will be trained\n",
        "        x : placeholder\n",
        "            the input\n",
        "        y_op : placeholder\n",
        "    \"\"\"\n",
        "    dp_dict = dict_to_one(network.all_drop)  # disable noise layers\n",
        "\n",
        "    prediction = np.zeros((seqlength))\n",
        "    if y_op is None:\n",
        "        y_op = network.outputs\n",
        "\n",
        "    banum = 0\n",
        "    datanum = 0\n",
        "    ave = np.ones((seqlength)) * output_length\n",
        "    ave[:output_length - 1] = np.arange(1, output_length)\n",
        "    ave[-(output_length - 1):] = np.arange(output_length - 1, 0, -1)\n",
        "\n",
        "\n",
        "    for batch in output_provider.feed(**out_kwag):\n",
        "        # print banum\n",
        "        banum += 1\n",
        "        X_out_a, gt_batch = batch\n",
        "        # print 'hi', X_out_a.mean()\n",
        "        fra_num = X_out_a.shape[0] / fragment_size\n",
        "        offset = X_out_a.shape[0] % fragment_size\n",
        "        # final_output = np.zeros((X_out_a.shape[0], output_length))\n",
        "        for fragment in xrange(fra_num):\n",
        "            x_fra = X_out_a[fragment * fragment_size:(fragment + 1) * fragment_size]\n",
        "            feed_dict = {x: x_fra, }\n",
        "            feed_dict.update(dp_dict)\n",
        "            batch_prediction = sess.run(y_op, feed_dict=feed_dict).reshape(-1, output_length) * std + mean\n",
        "            for sequence in batch_prediction:\n",
        "                prediction[datanum:datanum + output_length] += sequence\n",
        "                datanum += 1\n",
        "\n",
        "        if offset > 0:\n",
        "            feed_dict = {x: X_out_a[-offset:], }\n",
        "            feed_dict.update(dp_dict)\n",
        "            batch_prediction = sess.run(y_op, feed_dict=feed_dict).reshape(-1,output_length)\n",
        "            for sequence in batch_prediction:\n",
        "                prediction[datanum:datanum + output_length] += sequence\n",
        "                datanum += 1\n",
        "\n",
        "\n",
        "        # print 'hello', final_output.mean()\n",
        "    return prediction/ave\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}